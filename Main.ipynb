{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nmslib in /home/danish/Anacond/lib/python3.7/site-packages (2.0.5)\n",
      "Requirement already satisfied: psutil in /home/danish/Anacond/lib/python3.7/site-packages (from nmslib) (5.6.3)\n",
      "Requirement already satisfied: pybind11>=2.2.3 in /home/danish/Anacond/lib/python3.7/site-packages (from nmslib) (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /home/danish/Anacond/lib/python3.7/site-packages (from nmslib) (1.16.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import numpy\n",
    "import sys\n",
    "import nmslib\n",
    "import time\n",
    "import math\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd gdrive\n",
    "cd My\\ Drive\n",
    "cd eurlex4k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class DataSet:\n",
    "  def __init__(self,x_train_name,y_train_name):\n",
    "    self.x_train_name = x_train_name\n",
    "    self.y_train_name = y_train_name\n",
    "    self.X_train = self.read_X_dense(x_train_name)\n",
    "    self.Y_train = self.read_Y(y_train_name)\n",
    "  def read_X_dense(self,file_name):\n",
    "    with open(file_name) as reader:\n",
    "      dimension_line = reader.readline()\n",
    "      self.no_of_data_points = int(dimension_line.split(\" \")[0])\n",
    "      self.no_of_features = int(dimension_line.split(\" \")[1])\n",
    "      data = np.zeros((self.no_of_data_points,self.no_of_features))\n",
    "      row_number = 0\n",
    "      while True: \n",
    "        line = reader.readline() \n",
    "        if not line: \n",
    "            break\n",
    "        column_number = 0\n",
    "        for x in line.split(\" \"):\n",
    "          data[row_number][column_number] = float(x)\n",
    "          column_number+=1\n",
    "        row_number+=1\n",
    "      return data\n",
    "  def read_Y(self,file_name):\n",
    "    with open(file_name) as reader:\n",
    "      dimension_line = reader.readline()\n",
    "      no_of_data_points = int(dimension_line.split(\" \")[0])\n",
    "      self.no_of_labels = int(dimension_line.split(\" \")[1])\n",
    "      data = []\n",
    "      while True: \n",
    "        line = reader.readline() \n",
    "        if not line: \n",
    "            break\n",
    "        column_number = 0\n",
    "        data_row=[]\n",
    "        for x in line.split(\" \"):\n",
    "          data_row.append(int(x.split(\":\")[0]))\n",
    "        data.append(data_row)\n",
    "      return data\n",
    "  def clean_dataset(self):\n",
    "    remove_training_points_with_no_features()\n",
    "    remove_labels_with_no_training_data()\n",
    "    return\n",
    "  def remove_training_points_with_no_features(self):\n",
    "    final_X=[]\n",
    "    final_Y=[]\n",
    "    for i in range(self.no_of_data_points):\n",
    "      norm = 0\n",
    "      for j in range(self.no_of_features):\n",
    "        norm+=(self.X_train[i][j]**2.0)\n",
    "      if norm==0:\n",
    "        continue\n",
    "      final_X.append(self.X_train[i])\n",
    "      final_Y.append(self.Y_train[i])\n",
    "    self.X_train = np.array(final_X)\n",
    "    self.Y_train = final_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet('trn_ft_mat_dense.txt','trn_lbl_mat.txt')\n",
    "dataset.remove_training_points_with_no_features() #may be clean_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel:\n",
    "  def __init__(self,val):\n",
    "    self.val = val\n",
    "    self.coef_ = np.array([[0]])\n",
    "    self.intercept_ = np.array([val])\n",
    "  def predict_proba(y):\n",
    "    return np.full(len(y), val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HNSW:\n",
    "  def __init__(self,training_data,params):\n",
    "    self.training_data = training_data\n",
    "    self.M = params.param.M\n",
    "    self.efC = params.param.efC\n",
    "    self.efS = params.param.efS\n",
    "    self.metric_space = \"cosinesimil\"\n",
    "    self.num_threads = params.param.num_threads\n",
    "    self.num_nbrs = params.param.num_nbrs\n",
    "  def train(self,path):\n",
    "    self.index = nmslib.init(method='hnsw', space=self.metric_space)\n",
    "    self.index.addDataPointBatch(self.training_data)\n",
    "    self.index.createIndex({'M': self.M, 'indexThreadQty': self.num_threads, 'efConstruction': self.efC})\n",
    "    nmslib.saveIndex(self.index,path)\n",
    "\n",
    "  def find_closest_neighbours(self,data,label_to_datapoint_index):\n",
    "    self.index.setQueryTimeParams({'efSearch': self.efS, 'algoType': 'old'})\n",
    "    nbrs = self.index.knnQueryBatch(data, k=self.num_nbrs, num_threads = self.num_threads)\n",
    "    for i in range(len(data)):\n",
    "      for j in range(self.num_nbrs):\n",
    "        label_to_datapoint_index[nbrs[i][0][j]].append(i)\n",
    "  def load(self,model_file):\n",
    "    self.index = nmslib.init(method='hnsw', space=self.metric_space)\n",
    "    nmslib.loadIndex(self.index,model_file)\n",
    "  def evaluate_generative_model(self,data,model_file):\n",
    "    self.load(model_file)\n",
    "    self.index.setQueryTimeParams({'efSearch': self.efS, 'algoType': 'old'})\n",
    "    nbrs = self.index.knnQueryBatch(data, k=self.num_nbrs, num_threads = self.num_threads)\n",
    "    arr = []\n",
    "    for row in nbrs:\n",
    "      arr.append(row[0]) \n",
    "    return np.array(arr)\n",
    "    \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
