{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nmslib in /home/danish/Anacond/lib/python3.7/site-packages (2.0.5)\n",
      "Requirement already satisfied: psutil in /home/danish/Anacond/lib/python3.7/site-packages (from nmslib) (5.6.3)\n",
      "Requirement already satisfied: pybind11>=2.2.3 in /home/danish/Anacond/lib/python3.7/site-packages (from nmslib) (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /home/danish/Anacond/lib/python3.7/site-packages (from nmslib) (1.16.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import numpy\n",
    "import sys\n",
    "import nmslib\n",
    "import time\n",
    "import math\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd gdrive\n",
    "cd My\\ Drive\n",
    "cd eurlex4k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class DataSet:\n",
    "  def __init__(self,x_train_name,y_train_name):\n",
    "    self.x_train_name = x_train_name\n",
    "    self.y_train_name = y_train_name\n",
    "    self.X_train = self.read_X_dense(x_train_name)\n",
    "    self.Y_train = self.read_Y(y_train_name)\n",
    "  def read_X_dense(self,file_name):\n",
    "    with open(file_name) as reader:\n",
    "      dimension_line = reader.readline()\n",
    "      self.no_of_data_points = int(dimension_line.split(\" \")[0])\n",
    "      self.no_of_features = int(dimension_line.split(\" \")[1])\n",
    "      data = np.zeros((self.no_of_data_points,self.no_of_features))\n",
    "      row_number = 0\n",
    "      while True: \n",
    "        line = reader.readline() \n",
    "        if not line: \n",
    "            break\n",
    "        column_number = 0\n",
    "        for x in line.split(\" \"):\n",
    "          data[row_number][column_number] = float(x)\n",
    "          column_number+=1\n",
    "        row_number+=1\n",
    "      return data\n",
    "  def read_Y(self,file_name):\n",
    "    with open(file_name) as reader:\n",
    "      dimension_line = reader.readline()\n",
    "      no_of_data_points = int(dimension_line.split(\" \")[0])\n",
    "      self.no_of_labels = int(dimension_line.split(\" \")[1])\n",
    "      data = []\n",
    "      while True: \n",
    "        line = reader.readline() \n",
    "        if not line: \n",
    "            break\n",
    "        column_number = 0\n",
    "        data_row=[]\n",
    "        for x in line.split(\" \"):\n",
    "          data_row.append(int(x.split(\":\")[0]))\n",
    "        data.append(data_row)\n",
    "      return data\n",
    "  def clean_dataset(self):\n",
    "    remove_training_points_with_no_features()\n",
    "    remove_labels_with_no_training_data()\n",
    "    return\n",
    "  def remove_training_points_with_no_features(self):\n",
    "    final_X=[]\n",
    "    final_Y=[]\n",
    "    for i in range(self.no_of_data_points):\n",
    "      norm = 0\n",
    "      for j in range(self.no_of_features):\n",
    "        norm+=(self.X_train[i][j]**2.0)\n",
    "      if norm==0:\n",
    "        continue\n",
    "      final_X.append(self.X_train[i])\n",
    "      final_Y.append(self.Y_train[i])\n",
    "    self.X_train = np.array(final_X)\n",
    "    self.Y_train = final_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet('trn_ft_mat_dense.txt','trn_lbl_mat.txt')\n",
    "dataset.remove_training_points_with_no_features() #may be clean_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel:\n",
    "  def __init__(self,val):\n",
    "    self.val = val\n",
    "    self.coef_ = np.array([[0]])\n",
    "    self.intercept_ = np.array([val])\n",
    "  def predict_proba(y):\n",
    "    return np.full(len(y), val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HNSW:\n",
    "  def __init__(self,training_data,params):\n",
    "    self.training_data = training_data\n",
    "    self.M = params.param.M\n",
    "    self.efC = params.param.efC\n",
    "    self.efS = params.param.efS\n",
    "    self.metric_space = \"cosinesimil\"\n",
    "    self.num_threads = params.param.num_threads\n",
    "    self.num_nbrs = params.param.num_nbrs\n",
    "  def train(self,path):\n",
    "    self.index = nmslib.init(method='hnsw', space=self.metric_space)\n",
    "    self.index.addDataPointBatch(self.training_data)\n",
    "    self.index.createIndex({'M': self.M, 'indexThreadQty': self.num_threads, 'efConstruction': self.efC})\n",
    "    nmslib.saveIndex(self.index,path)\n",
    "\n",
    "  def find_closest_neighbours(self,data,label_to_datapoint_index):\n",
    "    self.index.setQueryTimeParams({'efSearch': self.efS, 'algoType': 'old'})\n",
    "    nbrs = self.index.knnQueryBatch(data, k=self.num_nbrs, num_threads = self.num_threads)\n",
    "    for i in range(len(data)):\n",
    "      for j in range(self.num_nbrs):\n",
    "        label_to_datapoint_index[nbrs[i][0][j]].append(i)\n",
    "  def load(self,model_file):\n",
    "    self.index = nmslib.init(method='hnsw', space=self.metric_space)\n",
    "    nmslib.loadIndex(self.index,model_file)\n",
    "  def evaluate_generative_model(self,data,model_file):\n",
    "    self.load(model_file)\n",
    "    self.index.setQueryTimeParams({'efSearch': self.efS, 'algoType': 'old'})\n",
    "    nbrs = self.index.knnQueryBatch(data, k=self.num_nbrs, num_threads = self.num_threads)\n",
    "    arr = []\n",
    "    for row in nbrs:\n",
    "      arr.append(row[0]) \n",
    "    return np.array(arr)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for row in neighbours:\n",
    "  arr.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datapoints_corresponsding_to_labels ---->   only postive ones\n",
    "#label_to_datapoint_index ----> all closest ones\n",
    "class Slice:\n",
    "  def __init__(self,dataset,param=None):\n",
    "    self.dataset = dataset\n",
    "\n",
    "  def train(self,is_saved):\n",
    "    print(\"Training Started\")\n",
    "    datapoints_corresponsding_to_labels = self.transpose_Y(self.dataset.Y_train)\n",
    "    print(\"Computing Mu_plus\")\n",
    "    self.Mu_plus = self.compute_mu_plus(datapoints_corresponsding_to_labels)\n",
    "    print(\"Mu_plus Computed\")\n",
    "    hnsw = HNSW(self.Mu_plus,params)\n",
    "\n",
    "    print(\"Training HNSW\")\n",
    "    if(is_saved):\n",
    "      hnsw.load('hsnw_model')\n",
    "    else:\n",
    "      hnsw.train('hsnw_model')\n",
    "    print(\"HNSW Trained\")\n",
    "\n",
    "    label_to_datapoint_index = [[] for i in range(len(self.dataset.X_train))]   \n",
    "    print(\"finding closest neighbours\")\n",
    "    hnsw.find_closest_neighbours(self.dataset.X_train,label_to_datapoint_index)\n",
    "    print(\"finding closest neighbours completed\")\n",
    "    print(\"unit normalize columns\")\n",
    "\n",
    "    self.dataset.X_train = self.unit_normalize_clolums(self.dataset.X_train)\n",
    "    print(\"1 vs All Training\")\n",
    "    return  self.dataset.X_train,label_to_datapoint_index,datapoints_corresponsding_to_labels\n",
    "    # models=[]\n",
    "    # for i in range(self.dataset.no_of_labels):\n",
    "    #   if i%50==0:\n",
    "    #     print(\"Training label number \",i)\n",
    "    #   x_train,y_train = self.get_positive_negative_datas(label_to_datapoint_index,i,datapoints_corresponsding_to_labels)\n",
    "    #   logisticRegr = LogisticRegression()\n",
    "    #   logisticRegr.fit(x_train, y_train)\n",
    "    #   models.append(logisticRegr)\n",
    "    #   if i%50:\n",
    "    #     print(\"Training label number \",i,\"Completed\")\n",
    "    #   try:\n",
    "    #     filename = 'models/model_'+str(i)+'.sav'\n",
    "    #     pickle.dump(logisticRegr, open(filename, 'wb'))\n",
    "    #   except:\n",
    "    #     print(\"some error in saving model \",str(i))\n",
    "  def get_positive_negative_datas(self,label_to_datapoint_index,label_no,datapoints_corresponsding_to_labels):\n",
    "    print(\"Get Positive Negative Datas Called\")\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    dict_postivies = {}\n",
    "    for i in range(len(datapoints_corresponsding_to_labels[label_no])):\n",
    "      datapoint_index = datapoints_corresponsding_to_labels[label_no][i]\n",
    "      x_train.append(self.dataset.X_train[datapoint_index])\n",
    "      y_train.append(1)\n",
    "      dict_postivies[datapoint_index]=1\n",
    "    for i in range(len(label_to_datapoint_index[label_no])):\n",
    "      try:\n",
    "        if(dict_postivies[label_to_datapoint_index[label_no][i]]!=None):\n",
    "          pass\n",
    "      except:\n",
    "        datapoint_index = label_to_datapoint_index[label_no][i]\n",
    "        x_train.append(self.dataset.X_train[datapoint_index])\n",
    "        y_train.append(-1)\n",
    "    return np.array(x_train),np.array(y_train)\n",
    "\n",
    "  def compute_mu_plus(self,datapoints_corresponsding_to_labels):\n",
    "    datapoints_added_for_every_label = self.add_features_for_labels(datapoints_corresponsding_to_labels)\n",
    "    normalized_datapoints_added_for_every_label = self.unit_normalize_clolums(datapoints_added_for_every_label)\n",
    "    return normalized_datapoints_added_for_every_label\n",
    "\n",
    "  def transpose_Y(self,Y):\n",
    "    transposed_result = [[] for i in range(self.dataset.no_of_labels)]\n",
    "    for i in range(len(Y)):\n",
    "      for j in range(len(Y[i])):\n",
    "        transposed_result[Y[i][j]].append(i)\n",
    "    return transposed_result\n",
    "  \n",
    "  def add_features_for_labels(self,datapoints_corresponsding_to_labels):\n",
    "    resultant_matrix = np.zeros((self.dataset.no_of_labels,self.dataset.no_of_features))\n",
    "    for i in range(self.dataset.no_of_labels):\n",
    "      for j in range(len(datapoints_corresponsding_to_labels[i])):\n",
    "        data_point_index = datapoints_corresponsding_to_labels[i][j]\n",
    "        for k in range(len(self.dataset.X_train[data_point_index])):\n",
    "          resultant_matrix[i][k]+=dataset.X_train[data_point_index][k]\n",
    "    return resultant_matrix\n",
    "  \n",
    "  def unit_normalize_clolums(self,datapoints_added_for_every_label):\n",
    "    rows = datapoints_added_for_every_label.shape[0]\n",
    "    colums = datapoints_added_for_every_label.shape[1]\n",
    "    for i in range(rows):\n",
    "      norm_sq = 0.0\n",
    "      for j in range(colums):\n",
    "        norm_sq+=datapoints_added_for_every_label[i][j]**2\n",
    "      norm_sq = sqrt(norm_sq)\n",
    "      if norm_sq==0:\n",
    "        norm_sq = 1\n",
    "      for j in range(colums):\n",
    "        datapoints_added_for_every_label[i][j]/=norm_sq\n",
    "    return datapoints_added_for_every_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "class Params:\n",
    "  def __init__(self,args):\n",
    "    self.parser = argparse.ArgumentParser()\n",
    "    self.parser.add_argument('-m', action=\"store\", dest=\"M\", type=int)\n",
    "    self.parser.add_argument('-c', action=\"store\", dest=\"efC\", type=int)\n",
    "    self.parser.add_argument('-s', action=\"store\", dest=\"efS\", type=int)\n",
    "    self.parser.add_argument('-k', action=\"store\", dest=\"num_nbrs\", type=int)\n",
    "    self.parser.add_argument('-o', action=\"store\", dest=\"num_io_threads\", type=int)\n",
    "    self.parser.add_argument('-t', action=\"store\", dest=\"num_threads\", type=int)\n",
    "    self.parser.add_argument('-C', action=\"store\", dest=\"classifier_cost\", type=float)\n",
    "    self.parser.add_argument('-f', action=\"store\", dest=\"classifier_threshold\", type=float)\n",
    "    self.parser.add_argument('-b', action=\"store\", dest=\"b_gen\", type=int)\n",
    "    self.parser.add_argument('-siter', action=\"store\", dest=\"classifier_maxiter\", type=int)\n",
    "    self.parser.add_argument('-q', action=\"store\", dest=\"quiet\", type=bool)\n",
    "    self.parser.add_argument('-stype', action=\"store\", dest=\"classifier_kind\", type=int)\n",
    "    self.param = self.parser.parse_args(args)\n",
    "  def get_params(self):\n",
    "    return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args = ['-m','100','-c','300','-s','300','-k', '300','-o', '20' ,'-t', '1', '-f' ,'0.000001','-C', '1','-siter' ,'20', '-b', '2' , '-stype','0' , '-q', '0']\n",
    "params = Params(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice  = Slice(dataset,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,label_to_datapoint_index,datapoints_corresponsding_to_labels = slice.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel:\n",
    "  def __init__(self,val):\n",
    "    self.val = val\n",
    "    self.coef_ = np.array([[0]])\n",
    "    self.intercept_ = np.array([val])\n",
    "  def predict_proba(y):\n",
    "    return np.full(len(y), val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "none_models = []\n",
    "def get_positive_negative_datas(X_train,label_to_datapoint_index,label_no,datapoints_corresponsding_to_labels):\n",
    "  x_train = []\n",
    "  y_train = []\n",
    "  dict_postivies = {}\n",
    "  for i in range(len(datapoints_corresponsding_to_labels[label_no])):\n",
    "    datapoint_index = datapoints_corresponsding_to_labels[label_no][i]\n",
    "    x_train.append(X_train[datapoint_index])\n",
    "    y_train.append(1)\n",
    "    dict_postivies[datapoint_index]=1\n",
    "  for i in range(len(label_to_datapoint_index[label_no])):\n",
    "    try:\n",
    "      if(dict_postivies[label_to_datapoint_index[label_no][i]]!=None):\n",
    "        pass\n",
    "    except:\n",
    "      datapoint_index = label_to_datapoint_index[label_no][i]\n",
    "      x_train.append(X_train[datapoint_index])\n",
    "      y_train.append(-1)\n",
    "  return np.array(x_train),np.array(y_train)\n",
    "\n",
    "count=0\n",
    "for i in range(dataset.no_of_labels):\n",
    "\n",
    "  if len(datapoints_corresponsding_to_labels[i])==0:\n",
    "    count+=1\n",
    "    none_models.append(i)\n",
    "    models.append(None)\n",
    "    continue\n",
    "\n",
    "  if i%500==0:\n",
    "    print(\"Training label number \",i)\n",
    "  x_train,y_train = get_positive_negative_datas(X_train,label_to_datapoint_index,i,datapoints_corresponsding_to_labels)\n",
    "  (unique, counts) = np.unique(y_train, return_counts=True)\n",
    "  logisticRegr = None\n",
    "  if(len(unique)==1):\n",
    "    logisticRegr = DummyModel(y_train[0])\n",
    "  else:\n",
    "    logisticRegr = LogisticRegression(penalty='l2')\n",
    "    logisticRegr.fit(x_train, y_train)\n",
    "  models.append(logisticRegr)\n",
    "  if i%500==0:\n",
    "    print(\"Training label number \",i,\"Completed\")\n",
    "  try:\n",
    "    filename = 'models/model_'+str(i)+'.sav'\n",
    "    pickle.dump(logisticRegr, open(filename, 'wb'))\n",
    "  except:\n",
    "    print(\"unique\",i)\n",
    "    print(\"some error in saving model \",str(i))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = DataSet('tst_ft_mat_dense.txt','tst_lbl_mat.txt')\n",
    "params_test_args=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def unit_normalize_clolums(datapoints_added_for_every_label):\n",
    "    rows = datapoints_added_for_every_label.shape[0]\n",
    "    colums = datapoints_added_for_every_label.shape[1]\n",
    "    for i in range(rows):\n",
    "      norm_sq = 0.0\n",
    "      for j in range(colums):\n",
    "        norm_sq+=datapoints_added_for_every_label[i][j]**2\n",
    "      norm_sq = sqrt(norm_sq)\n",
    "      if norm_sq==0:\n",
    "        norm_sq = 1\n",
    "      for j in range(colums):\n",
    "        datapoints_added_for_every_label[i][j]/=norm_sq\n",
    "    return datapoints_added_for_every_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def evaluate_discriminative_model(score_mat_gen,x_train,models,k,params):\n",
    "  gamma=0\n",
    "  for model in models:\n",
    "    if model is not None:\n",
    "      weight_array = np.hstack((model.intercept_[:,None], model.coef_))[0]\n",
    "      temp = 0\n",
    "      for num in weight_array:\n",
    "        temp+=num**2.0\n",
    "      gamma+=sqrt(temp)\n",
    "  gamma = gamma/(len(models)-len(none_models))\n",
    "  print(gamma)\n",
    "  m=0\n",
    "  scores = []\n",
    "  for tst in x_train:\n",
    "    tst = np.append(1, tst)\n",
    "    pred_scores = []\n",
    "    for model_no in score_mat_gen[m]:\n",
    "        weight_array = np.hstack((models[model_no].intercept_[:,None], models[model_no].coef_))[0]\n",
    "        prod = np.dot(tst,weight_array)\n",
    "        score = 1.0/(1.0+math.exp(-prod)) + 1.0/(1.0+math.exp(-prod*gamma+params.param.b_gen))\n",
    "        pred_scores.append((model_no,score))\n",
    "    pred_scores.sort(key = lambda x: x[1],reverse=True)\n",
    "    if (k > len(pred_scores)):\n",
    "      k = len(pred_scores)\n",
    "    pred_scores = pred_scores[:k]\n",
    "    scores.append(pred_scores)\n",
    "    m+=1\n",
    "  return scores\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
